{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Frank-Wolfe\n",
    "\n",
    "From Reddi et al."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO LIST\n",
    "\n",
    "### Most important\n",
    "1) Come calcolare argmax?\n",
    "\n",
    "3) Inserisco calcolo gamma\n",
    "4) Metto come parametri \"dati\" del modello la gamma e la T specificate nel teorema di convergenza\n",
    "5) E' da inserire un early stopping? Teoricamente nello pseudocodice non c'e, ma se e' per questo non c'e' nemmeno nell'FW normale.\n",
    "\n",
    "6) Aggungere pseudocodice\n",
    "7) Aggiungere sezione oracle?\n",
    "8) Altre cose da aggiungere?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Frank-Wolfe methods (in convex case) have gained recent interest in machine learning and optimization communities due to their projection-free property and their ability to explout structured constraints. However, our understanding of these algorithms in the nonconvex setting is fairly limited. Given the vast importance of nonconvex models in machine learning, we propose a stochastic Frank-Wolfe algorithm.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "**Optimization problem.** We study optimization problem of the form:\n",
    "\n",
    "$$ \\underset{x \\in \\Omega}{min} \\  F(x) = \\mathbb{E_z}[f(x,z)] $$\n",
    "\n",
    "We assume:\n",
    "-  $F$ differentiable, possibly nonconvex, and *L-smooth*, i.e., its gradient is Lipschitz continuous with constant L: $$ || \\nabla F(x) - \\nabla F(y) || \\leq L||x-y||, \\ \\ \\ \\ \\ \\forall x,y \\in \\Omega$$ where $||.||$ denotes the $\\ell^2$ norm.\n",
    "\n",
    "-  $f$ differentiable, possibly nonconvex, and $G$-Lipschitz, i.e.: $$ || \\nabla f(x,z) || \\leq G, \\ \\ \\ \\ \\ \\forall x \\in \\Omega, z \\in \\Xi $$ where $||.||$ denotes the $\\ell^2$ norm.\n",
    "\n",
    "- $\\Omega$ is convex and compact.\n",
    "\n",
    "**Convergence criteria.** As convergence criteria, we use the following quantity, typically referred to as *Frank-Wolfe gap*:\n",
    "\n",
    "$$G(x) = \\underset{v \\in \\Omega}{max} \\langle v-x, -\\nabla F(x) \\rangle$$\n",
    "\n",
    "In nonconvex functions, G(x) = 0 if and only if x is a stationary point. To state our converge results, we will also need the following bound:\n",
    "\n",
    "$$\\beta \\geq \\frac{2(F(x_0)-F(x*))}{LD^{2}}$$\n",
    "\n",
    "given some (unspecified) initial point $x_0 \\in \\Omega$.\n",
    "\n",
    "**Oracle model.** Has to be added? It is used to compare speed of different algorithms, but we implement only one algorithm.\n",
    "\n",
    "## SFW algorithm\n",
    "\n",
    "Follows the pseudocode of the SFW algorithm.\n",
    "\n",
    "**TO COMPLETE**\n",
    "\n",
    "- Samples {z_i} has been chosen independentely according to the distribution *P*. Thus, we obtain an unbiased estimate of the gradient.\n",
    "- The step sizes $\\{\\gamma_i\\}^{T-1}_{i=0}$ and the minibatch sizes $\\{b_t\\}$ are the key parameters of the model, and must be chosen appropriately in order to ensure convergence of the algorithm. SEE THE CONVERGENCE RESULT\n",
    "- Note that the output is randomly selected from all the iterates of the algorithm.\n",
    "\n",
    "The following key result applies for nonconvex SFW.\n",
    "\n",
    "**Theorem X.** *Consider the presented SFW algorithm and its described stochastic setting. Then, the output* $x_a$ *with parameters* $\\gamma_t = \\gamma = \\sqrt{\\frac{2(F(x_0)-F(x^{*}))}{TLD^2 \\beta}}, b_t = b = T$ *for all* t \\in {0,...,T-1}, *satisfies the following bound:*\n",
    "\n",
    "$$ \\mathbb{E}[G(x_a)] \\leq \\frac{D}{\\sqrt{T}} \\left( G + \\sqrt{\\frac{2L(F(x_0)-F(x^{*}))}{\\beta}}(1+\\beta)\\right)$$\n",
    "\n",
    "*where* x^{*} *is an optimal solution to the stochastic problem described*.\n",
    "\n",
    "An immediate consequence of Theorem X is the following complexity result for SFW.\n",
    "\n",
    "**Corollary X.**. *Under the setting of Theorem X, the SFO complexity and LO complexity of SFW algorithm are* O(1/ $\\varepsilon^{4}$ ) *and* O(1/$\\varepsilon^{2}$) *respectively*.\n",
    "\n",
    "Note the SFO and LO complexity of nonconvex SFW is slightly worse than complexity of SFW for convex problems. Note also that we used a fixed step size and minibatch size. One can derive an essentially similar result using a decreasing step size and an increasing minibatch size.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION WITH CONSTRAINT\n",
    "\n",
    "I implemented the linear regression at the end of Combettes\n",
    "\n",
    "## -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_SFW_gradient(xk, sample, y_batch):\n",
    "    grad = (np.shape(sample)[0])^(-1) * (y_batch - np.matmul(sample,xk))^2\n",
    "    return grad\n",
    "\n",
    "def linear_SFW_argmax(grad):\n",
    "    # What is the element in the constrained set which maximise the multiplication between itself and -gradient?\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def linear_SFW_predict(xt, X_test):\n",
    "    return np.matmul(X_test,xt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the proper SFW algorithm.\n",
    "\n",
    "Notation conversion from pseudo-code to code:\n",
    "- $x_0$ stays the same;\n",
    "- *T* (number of iterations) becomes *epochs*;\n",
    "- *gamma_i* stays the same (you give an input a specified rule and, depeding on it, gamma's value is computed)\n",
    "- *b_i* becomes *batch_dim*.\n",
    "\n",
    "It also add the following parameters:\n",
    "- *sample*, *y_true*: the sample covariates and outputs, respectively;\n",
    "- *task*: depending on the input, it performs a different task.\n",
    "- *check*: ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_SFW(samp, y_true, x0, batch_dim, check, gamma = \"theorem\", epochs=101):\n",
    "\n",
    "    # 1. Choose the starting point\n",
    "    xt = []\n",
    "    xt.append(x0)\n",
    "\n",
    "    # this section add a column of\n",
    "    # 1 in front of the sample (to compute the gradient of beta0)\n",
    "    samp = np.concatenate((np.ones((samp.shape[0], 1)), samp), axis=1)\n",
    "\n",
    "    acc_list = []\n",
    "    time_list = []\n",
    "\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    for t in range(epochs):\n",
    "\n",
    "        # Uniformly randomly pick i.i.d. samples {z1, ..., z}\n",
    "        indices = np.random.choice(samp.shape[0], batch_dim, replace=False)\n",
    "        y_batch = y_true[indices]\n",
    "        samp_batch = samp[indices]\n",
    "\n",
    "        # 3. Compute the search direction\n",
    "        grad = linear_SFW_gradient(xt[t], samp_batch, y_batch)\n",
    "        vt = linear_SFW_argmax(-grad)\n",
    "\n",
    "        # NA. Compute the step size\n",
    "        if gamma == \"diminishing\":\n",
    "            gam = 1/(t+1)\n",
    "        # elif gamma == \"theorem\":\n",
    "            # gam = TO COMPLETE\n",
    "\n",
    "        # Compute the update direction and update the iterate:\n",
    "        dt = vt - xt[t]\n",
    "        xt.append(xt[t] + gam*dt)\n",
    "\n",
    "        # Store accuracy and time\n",
    "        if(t % check == 0):\n",
    "            mse = mean_squared_error(y_true, np.matmul(samp, xt[t]))\n",
    "            acc_list.append(mse)\n",
    "            time_list.append(time.process_time() - start_time)\n",
    "\n",
    "            print(f\"The MSE at iteration {t} is: {mse}\")           \n",
    "\n",
    "    index = np.random.choice(range(epochs))\n",
    "    x_fin = xt[index]\n",
    "    return x_fin, np.array(acc_list), np.array(time_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section use SFW on a linear regression task and compare it to the result obtained through sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE at iteration 0 is: 29711.32294617564\n",
      "The MSE at iteration 10 is: 29680.585637393768\n",
      "The MSE at iteration 20 is: 29680.585637393768\n",
      "The MSE at iteration 30 is: 29680.585637393768\n",
      "The MSE at iteration 40 is: 29680.585637393768\n",
      "The MSE at iteration 50 is: 29680.585637393768\n",
      "The MSE at iteration 60 is: 29680.585637393768\n",
      "The MSE at iteration 70 is: 29680.585637393768\n",
      "The MSE at iteration 80 is: 29680.585637393768\n",
      "The MSE at iteration 90 is: 29680.585637393768\n",
      "The MSE predicted by Sklearn is: 2798.193485169719\n",
      "The MSE predicted by SFW is: 26519.439213483143\n"
     ]
    }
   ],
   "source": [
    "batch_dim = 15\n",
    "lamb = 0.1\n",
    "gamma = \"diminishing\"\n",
    "epochs = 100\n",
    "check = 10\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "x0 = np.zeros(X_train.shape[1]+1)\n",
    "\n",
    "# Compute Lasso regression both with SFW and scikit\n",
    "SFW_weights, acc, tempo = linear_SFW(X_train, y_train, x0, batch_dim, check, gamma, epochs)\n",
    "SFW_pred = np.matmul(np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1), SFW_weights)\n",
    "SFW_mse = mean_squared_error(y_test, SFW_pred)\n",
    "\n",
    "sklearn_pred = Lasso(alpha = lamb).fit(X_train,y_train).predict(X_test)\n",
    "sklearn_mse = mean_squared_error(y_test, sklearn_pred)\n",
    "\n",
    "print(f\"The MSE predicted by Sklearn is: {sklearn_mse}\\nThe MSE predicted by SFW is: {SFW_mse}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENATE SUL LASSO - Per ora non me la sento ancora di buttarle :)\n",
    "\n",
    "The following section implements gradient and argmax computation in case of SFW is used for LASSO regression. We chose LASSO regression as it can be implemented as a linear program.\n",
    "\n",
    "LASSO's loss function:\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{2n} \\cdot (y-X\\beta)^{2} + \\lambda \\cdot ||\\beta||_{1}$$\n",
    "\n",
    "Thus, the j-th partial derivative of the loss is:\n",
    "\n",
    "$$ \\frac{\\partial L(\\beta)}{\\partial \\beta_j} =  \\frac{1}{n} \\cdot (y-X\\beta)^{2} \\cdot x_{j} + sign(\\lambda)$$\n",
    "\n",
    "As we said, LASSO can be implemented as a linear program. If we choose this way, the optimization problem becomes:\n",
    "\n",
    "$$ \\textup{minimize} \\ \\ \\  \\frac{1}{2n} \\cdot (\\y-X\\beta)^2$$\n",
    "$$ \\textup{subject to} \\ \\ \\  \\beta \\geq 0 \\textup{and} X^{T} \\cdot (y-X\\beta)$$\n",
    "Where t is a scalar parameter that controls the sparsity of the solution. This is basically the Ordinary Least Squares problem subject to the constraint.\n",
    "\n",
    "The j-th partial derivative of the loss becomes:\n",
    "\n",
    "$$ -2 \\cdot ||y - X\\beta|| \\cdot x_j $$\n",
    "\n",
    "Link to the guy which implemented \"argmin\" original function:\n",
    "https://github.com/paulmelki/Frank-Wolfe-Algorithm-Python/blob/master/frank_wolfe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_SFW_gradient(xk, sample, y_batch):\n",
    "    # STANDARD IMPLEMENTATION\n",
    "    # grad = (1/m) * X^T * (X * xk - y_true) + lambda * sign(xk)\n",
    "    #grad = (1/sample.shape[0]) * np.matmul(sample.T, (np.matmul(sample,xk) - y_batch)) \\\n",
    "    #    + lamb*np.sign(xk)\n",
    "\n",
    "    # LINEAR PROGRAMMING IMPLEMENTATION\n",
    "    grad = -2 * \n",
    "    return grad\n",
    "\n",
    "def lasso_SFW_argmax(grad):\n",
    "\n",
    "    # ???\n",
    "\n",
    "    # Initialize the zero vector\n",
    "    xk_hat = np.zeros(len(grad))\n",
    "\n",
    "    # Check if all coordinates of x are 0\n",
    "    # If they are, then the Oracle contains zero vector\n",
    "    if (grad != 0).sum() == 0:\n",
    "        return xk_hat\n",
    "\n",
    "    # Otherwise, follow the following steps\n",
    "    else:\n",
    "        # Compute the (element-wise) absolute value of x\n",
    "        a = abs(grad)\n",
    "        # Find the first coordinate that has the maximum absolute value\n",
    "        i = np.nonzero(a == max(a))[0][0]\n",
    "        # Compute s\n",
    "        xk_hat[i] = - np.sign(grad[i]) * lamb\n",
    "        return xk_hat\n",
    "\n",
    "def lasso_SFW_predict(xt, X_test):\n",
    "    return np.matmul(test_1,xt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
